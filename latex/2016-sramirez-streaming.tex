%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[hyphens]{url}
\usepackage{multirow}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{color}
\usepackage{colortbl}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Knowledge Based Systems}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A Survey on Streaming Data Preprocessing}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author[ugr]{S. Ram\'{i}rez-Gallego\corref{cor1}}
\ead{sramirez@decsai.ugr.es}

\author[ugr]{S. Garc\'ia}
\ead{salvagl@decsai.ugr.es}

\author[pwr]{B. Krawczyk}
\ead{bartosz.krawczyk@pwr.edu.pl}

\author[pwr]{M. Wo\'zniak}
\ead{michal.wozniak@pwr.edu.pl}

\author[ugr]{F. Herrera}
\ead{herrera@decsai.ugr.es}

\address[ugr]{Department of Computer Science and Artificial Intelligence, CITIC-UGR, University of Granada, 18071 Granada, Spain}
	
\address[pwr]{Department of Computer Science, Wroc\l{}aw University of Technology, Wyb. Wyspianskiego 27, 50-370 Wroc\l{}aw, Poland}

\cortext[cor1]{Corresponding author}

\begin{abstract}
%% Text of abstract

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

machine learning \sep data streams \sep data preprocessing

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:intro}



\section{Data streaming and concept drift}


\section{Feature selection for data streaming}
\label{sec:fs}

Many feature selection algorithms for streaming data have been proposed in the literature. Most of them are naturally incremental methods but others can process streams in an online fashion. Also depending on what set is getting new elements, we can make a further distinction between methods. Some feature selection methods consider that feature arrive one by one while feature vectors (examples) are always available~\citep{wu10, eskandari16}. In contrast, other online methods assume that the instances always arrive sequentially and the feature set can change or not. The last approach is more natural in real-world problems so we will focus on these methods. 

In streaming learning, the feature space is also affected by the changes in data distribution. \textbf{Feature drifts} occur whenever the relevance of a given attribute $A_i$ changes over time when new instances arrive at the system~\cite{barddal15}. Let $rel(A_i, t_i)$ be the relevance function that determines the importance of $A_i$ in $t_i$ time of the stream. There exits a feature drift iff $rel(A_i)$ changes between $t_j$ and $t_k$ timestamps.

\begin{equation}\label{eq:rel}
\exists t_j, \exists t_k, t_j < t_k, rel(A_i, t_j) \neq rel(A_i, t_k)
\end{equation}

As in other concept drifts, changes in relevance enforce algorithms to discard or adapt the model already learned by removing the most irrelevant features in the new scenario, as well as including the most relevant ones (dynamic feature selection)~\cite{nguyen12}. As changes in relevance directly affects the decision boundaries, feature drift can be seen as a specific type of real concept drift, which is independent of fluctuations in the data distribution $P[x]$. 

As the set of selected features evolves over time, it is likely that the feature space in test instances be distinct from the current selection. Therefore, when a new instance is classified, we need to perform a conversion between feature spaces for homogenization purposes~\cite{masud10}. The type of conversion to consider are the followings: 
\begin{itemize}
	\item Lossy Fixed (Lossy-F): the same feature set is used for the whole stream. It is generated from the first batch. All the following instances (training and test) will be mapped to this set, resulting in a clear loss in future importance.
	\item Lossy Local (Lossy-L): a different feature space is used for each new training batch. Test instances are thus mapped to the training space in each iteration. This conversion is also troublesome because features in test that can be relevant are omitted.
	\item Lossless Homogenizing (Lossless): Lossless is similar to the previous case, except that here the feature space in the test set is considered in the conversion. There exist a homogenization between spaces, for example, by unifying both spaces and padding with zeros any missing feature in the other set. This conversion deems both space and time, so it can be seen as the best alternative.
\end{itemize}

In the followings, we enumerate those algorithms that address the streaming feature selection problem. Table~\ref{tab:fs} details the type of selection method and the space conversion performed by each algorithm.

\begin{itemize}
	\item Katakis et al.~\cite{kata05} was among the first to introduce the problem of dynamic feature space over time in data streams. They proposed a technique that includes a feature ranking (filter) method to select relevant features. As the importance score of each feature can be measured using many cumulative functions like information gain, $\chi^2$ or mutual information; it is a versatile solution for feature ranking.
	\item Carvalho et al.~\cite{carva06} proposed Extremal Feature Selection (EFS), an online feature selection method that uses the weights computed by an online classifier (Modified Balance Winnow) to measure the relevance of features. The score is computed as the absolute difference between the positive and negative weights for each feature.
	\item Wenerstrom and Giraud-Carrier~\cite{wener06} proposed a dynamically-sized ensemble algorithm (called FAE) composed of learners with different feature space sets. Learners are created when the top features changes over a threshold or when the ensemble's performance degrades too much. FAE is also based on a feature ranking method that cumulates $\chi^2$-based scores. FAE performs a Lossy-L conversion between spaces. %According to the author's experiments, FAE performs better than the Katakis's approach. %Lossy-L conversion
	\item Masud et al.~\cite{masud10} proposed a data stream classification technique (DXMiner), which uses the deviation weight measure to rank features in a single pass. Furthermore, DXMiner naturally address the problem of novel classes (concept-evolution) by building a decision boundary around the training data. In contrast to previous methods, DXMiner uses lossless conversion, which is useful to detect novel classes. To rank features in the test feature space, DXMiner uses a unsupervised technique (e.g., the highest frequency).
	\item Nguyen et al.~\cite{nguyen12} use an ensemble technique based on windowing to detect feature drifts. The algorithm is based on a ensemble of classifiers, where each classifier has its own feature set. If a drift is detected, the ensemble is updated with a new classifier together with a new feature subset; otherwise, each classifier is updated accordingly. Fast Correlation-Based Filter (FCBF) is the feature selection method used in this method. FCBF heuristically applies a backward selection technique with a sequential search strategy to remove irrelevant and redundant features.
	\item In~\cite{gomes14}, the authors propose an algorithm to mine recurring concepts (called MReC-DFS). Here, the same online selector method proposed in~\cite{kata05} is used. Instead of selecting a fixed number of features, they propose to use either a fixed threshold or an adaptive one based on percentiles. They also compare the effects of using different space conversions~\cite{masud10} (like Lossy-F, Lossy-L or Lossless).
	\item Wang et al.~\cite{wang14} proposed an $\varepsilon$-greedy online feature selection method (called OFS) based on a classical technique that makes a trade-off between exploration and exploitation of features. The algorithm spends $\varepsilon$ trials on exploration by randomly choosing $N$ attributes from the whole set of attributes, and the remaining 1-$\varepsilon$ trials on exploitation by choosing the $N$ attributes for which the linear classifier has nonzero values. %This selection method also presents a lossy adaptation since the feature space of the test test is not considered in the selection process.
In this work, no feature drift is addressed explicitly, and no comparison with previous works is performed.
\end{itemize}

\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Summary description of streaming feature selection methods. Information about the type of selector (wrapper or filter) and space conversion used is included.}
\label{tab:fs}
\begin{tabular}{ lcc }
\toprule
{\bf Method} & {\bf Type of selector} & {\bf Space conversion}\\
\midrule
Katakis's method~\cite{kata05} & Filter ($\chi^2$) & Lossy-L\\
EFS~\cite{carva06} & Wrapper (classifier's weights) & Lossy-L\\
FAE~\cite{wener06} & Filter ($\chi^2$) & Lossy-L\\
DXMiner~\cite{masud10} & Wrapper (deviation weight + unsupervised) & Lossless\\
HEFT-Stream~\cite{nguyen12} & Filter (FCBF) & Lossy-F, Lossy-L, Lossless\\
MReC-DFS~\cite{gomes14} & Filter ($\chi^2$) & all\\
OFS~\cite{wang14} & Wrapper (classifier's weights) & Lossy-L\\
\bottomrule
\end{tabular}
\end{table}


%  \begin{tabular}{lcc}
%  \toprule {\bf Method} & {\bf Type of selector} & {\bf Space conversion}\\
%\midrule
%Katakis's method~\cite{kata05} & Filter ($\chi^2$) & -\\
%EFS~\cite{carva06} & Wrapper (classifier weights) & -\\
%FAE~\cite{wener06} & Filter ($\chi^2$) & Lossy-L\\
%DXMiner~\cite{masud10} & Wrapper (deviation weight + unsupervised) & Lossless\\
%HEFT-Stream~\cite{nguyen12} & Filter (FCBF) & -\\
%MReC-DFS~\cite{gomes14} & Filter ($\chi^2$) & all\\
%OFS~\cite{wang14} & Wrapper (classifier weights) & -\\
%	\bottomrule
%  \end{tabular}

%Finally, in~\cite{shar06}, the authors presented an algorithm for efficiently performing threshold function queries to manage \textbf{distributed data streams}. This function queries are typically used for feature selection. The algorithms are based on a geometric analysis of the problem. Upon initialization, the algorithm collects frequency counts from all the streams, and calculate the initial result of the query. In addition, a numerical constraint on the data received on each individual stream is defined. As data arrives on the streams, each node verifies that the constraint on its stream has not been violated. The geometric analysis of the problem guarantees that as long as the constraints on all the streams are upheld, the result of the query remains unchanged, and thus no communication is required. If a constraint on one of the streams is violated, new data is gathered from the streams, the query is reevaluated, and new constraints are set on the streams. 

\section{Instance selection for data streaming}

Lazy learning algorithms have been broadly used to address several problems in machine learning. However, case-bases naturally deteriorate and grow with usage over time, which affects data distribution and the decision boundaries underlying data.

In this scenario, past preserved cases that belong to a previous concept may degrade the performance of the learner if a new concept appears. Likewise, new instances that represents a new concept may be classified as noise and removed by the edition mechanism, because they disagree with past concepts. 

Some enhancement and maintenance should be thus performed on case-bases through the usage of sophisticated instance selection algorithms that select those cases that represent more accurately the current concept. Nevertheless, most of current techniques assume stationarity in data and ignore the concept drift phenomenon. Next, we enumerate those instance selection techniques that explicitly address the drift when selecting instances:

\begin{itemize}
	\item Instance-Based learning Algorithm 3 (IB3)~\cite{aha91} is one of the first attempts to deal with concept drift. It is based on accuracy and retrieval frequency measures (per instance). By means of a confidence interval test, IB3 decides if a case should be added to the case-base or it needs to wait until its insertion is appropriate. The removal of a case is performed if its accuracy is below (in a degree) its class's frequency. Due to IB3 defers the inclusion of examples, it is only suitable for gradual concept drift. 
	\item The Locally Weighted Forgetting (LWF) algorithm~\cite{salga93} is an instance weighting technique based on k-nearest neighbors (k-NN). In LWF, those cases with a weight below a threshold are removed. LWF algorithm has been criticized by its lower asymptotic classification in static environments and by its tendency to over-fitting~\cite{klinken04}. This method has shown a good performance for both gradual and sudden concept drift. 
	\item Salganicoff~\cite{salga97} designed the Prediction Error Context Switching (PECS) algorithm, which is designed to work in both dynamic and static environments. PECS algorithm is based on the case's accuracy measure used in IB3, adopting the same confidence test. In order to introduce the time factor in the decisions, PECS only consider the latest predictions to compute accuracy. Furthermore, PECS immediately add new cases to the base to expedite the slow adaptation process. Finally, PECS disables cases instead of permanently deleting them. Those cases can be re-introduced if their accuracy get high again. It is argued in~\cite{berin07} that the main drawbacks of PECS are its high memory usage and its slow removal process (new instances are always retained at first).
	\item Delany et al.~\cite{delany05} proposed an drift control mechanism with two levels. The method in the first level is an hybrid of two competence-based editing methods\footnote{Basic concepts about competence models can be reviewed in~\cite{smyth95}}: Blame Based Noise Removal (BBNR) and Conservative Redundancy Reduction (CRR). BBNR is aimed at deleting those cases whose removal do not imply coverage loss, whereas CRR selects the cases with the smallest coverage and that are misclassified. Note that both methods are designed for static problems, which can cause some problems like the removal of novel concepts when gradual drift appears, and the inability of removing small groups of cases where examples covers each other but misclassifies all the surrounding neighbors. Furthermore, BBNR do not keep the competence model updated, but it needs to completely rebuild the competence model in the second level. An outdated competence model may yield inconsistencies in the evaluation phase as this model does not reflect the current concept accurately.
	\item Instance-Based Learning on Data Streams (IBL-DS)~\cite{berin07} algorithm is presented as the first solution that deems both time and space factors to control the shape and size of the case-base. In IBL-DS, every neighbor in a test range is removed if the new case's class dominates in this range. IBL-DS also introduces an explicit drift detection method developed by Gama~\cite{gama04}, which determines when removing a fixed number of instances considering both space and time factors. The number of cases to be removed is computed considering the minimum error rate and the error of the last classification stages. IBL-DS controls the size of the case-base by removing the oldest instances. However, the time-based removal strategy implemented by IBL-DS has been criticized because some old but relevant instances may be eliminated in this process.
	\item FISH algorithms~\cite{zlio11} are also based on a combination of time and space factors, in this case, in form of distances. The idea behind these algorithms is to dynamically select the most relevant examples for training the next model in the ensemble. Three different versions of FISH were proposed. In FISH1, the training size is fixed at the start. FISH2 selects the best training size according to the accuracy (through leave-one-out cross validation). FISH3 also weights time and space by using a different loop of cross validation.	
	\item Lu et al.~\cite{lu16} propose a case-base editing technique based on competence preservation and enhancement~\cite{smyth95}. Their solution consists of three stages: the first one compares the distribution between two windows in order to detect if there is a drift or not. Apart from detecting the drift, this method also limit the area where the distribution changes most. Afterwards, the Noise-Enhanced Fast Context Switch (NEFCS) method is applied. NEFCS examine all new cases and determine whether they are noise or not (enhancement). However, only the noisy cases that lie outside the detected competence areas are removed, as they may be part of a novel and useful concept. Finally, Stepwise Redundancy Removal (SRR) method is aimed at controlling the size of the case-base (preservation). SRR removes redundant examples recursively until the case-base's coverage gets deteriorated.

\end{itemize}


\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Summary description of streaming instance selection methods. Information about the type of selection and whether drift detection is used or not is shown here.}
\label{tab:fs}
\begin{tabular}{ lcc }
\toprule
{\bf Method} & {\bf Type of selector} & {\bf Drift detection}\\
\midrule
IB3~\cite{aha91} & case accuracy & no\\
LWF~\cite{salga93} & weighting & no\\
PECS~\cite{salga97} & case accuracy & no\\
BBNR+CRR~\cite{masud10} & competence & no\\
IBL-DS~\cite{berin07} & time-space distance & yes\\
FISH~\cite{gomes14} & time-space distance & no\\
NEFCS+SRR~\cite{lu16} & competence & yes\\
\bottomrule
\end{tabular}
\end{table}

%
%%Despite this, the PECS algorithm differs from IB3 in several ways: First, any new observation is immediately included in the case-base. Second, the PECS algorithm calculates a case's accuracy based only on its latest $l$ predictions. Third, rather than permanently deleting a case as noise, the PECS algorithm deactivates it and tracks its accuracy for reactivation purposes. Experiments show that the algorithm improves robustness over IB3 on time-varying tasks at the cost of increased storage requirements. However, PECS was originally designed to improve performance in concept drift problems, where noise along with incoming observations was not considered. As a result, all noisy observations are retained first and can only be removed with a deferment. PECS has also been criticized for its unlimited memory assumption~\cite{berin07}, by only disabling cases but not deleting them. 
%
%%Delany et al.~\cite{delany05} suggested a two-level learning for handling concept drift. In level-1, they used a Competence-Based Editing (CBE) method, which is a hybrid of Blame Based Noise Removal (BBNR) and Conservative Redundancy Reduction (CRR) to manage the case-base periodically. Specifically, BBNR analyzes all cases that have contributed to misclassification and removes cases if their deletion results in no coverage loss; CRR repeatedly selects a case with the smallest coverage set that cannot be correctly solved. 
%
%%However, a hybrid of two CBM methods designed for a static environment does not guarantee effective learning under concept drift. In the worst case, novel concepts can be consistently discarded, especially with gradual concept drift. In addition, the BBNR algorithm has difficulties in removing small groups of noisy cases. For example, two noisy cases that have each other in their coverage sets can correctly classify each other but can cause the misclassification of all other nearby cases; these small groups of noisy cases can never be removed by the BBNR, even though they continue to provide incorrect classification results. This phenomenon can be caused by outdated cases when concept drift occurs. 
%
%Last but not least, BBNR neglects the competence model maintenance issue. Referring to an ill-matched competence model may lead to the mistaken preservation of noisy cases, i.e., if we have previously removed a noisy case $c$ that happens to be a member of the coverage set $c$ that we are currently considering. If we do not keep the competence model up-to-date, $c$ will still be considered even though we know it is a noisy case. In level-2 learning, Delany et al. periodically reselected features to completely rebuild a CBR system. %Level-2 learning~\cite{delany05} is beyond the scope of a case-base editing method, but it is a model rebuilding strategy that can be plugged into any instance selection technique. Later studies conducted by Delany and Bridge~\cite{delany07} suggested a feature-free distance measure which showed further improvement in accuracy on the same datasets in their experiments.
%
%Beringer and H\"ullermeier~\cite{berin07} presented an Instance-Based Learning on Data Streams (IBL-DS) algorithm that autonomously controls the composition and size of the case-base. This IBL-DS algorithm is based on three modification rules: 1) when the size of the case-base exceeds its limit, the oldest instance will be removed; 2) when concept drift is reported using Gama's detection method~\cite{gama04}, a large number of instances will be deleted in a spatially uniform but temporally skewed way. The number of cases to be removed depends on the discrepancy between the minimum error rate and the error rate of the 20 most recent classifications; 3) for every retained new case whose class dominates in a test range, all neighbors in a candidate range that belong to a different class will be removed. 
%
%Although Beringer and H\"ullermeier's method is instructive in that it differentiates its instance selection strategy based on whether concept drift is reported, their instance deletion strategy might be difficult to implement in problem domains such as spam filtering where all features may be binary. Removing cases temporally may also result in loss of case-base competence, e.g., deleting rare but correct cases.
%
%In~\cite{zlio11}, the authors invented a family of training set formation methods named FISH (uniFied Instance Selection algoritHm) based on ensembles of classifiers. The FISH family dynamically selects a set of relevant instances as the training set for the current target instance. The key concept of the FISH family is to linearly combine distances in time and feature space for training set selection. It includes three modifications: FISH1, FISH2 and FISH3. In FISH1, the size of a training set is fixed and set in advance. FISH2, which is considered to be central to the family, varies the size by selecting a training set that achieves the best accuracy based on leave-one-out cross validation. In FISH3, the relative importance of time and space distance is determined through an additional loop of cross validation. Although the FISH family is reported to be capable of cooperating with any base classifier, the validation set is chosen based on nearest neighbors to the current target instance. 
%
%Lu et al.~\cite{lu16} propose a case-base editing technique based on competence preservation and enhancement~\cite{smyth95}. Their solution consists of three modules: 1) competence-based drift detection, which compares the case distribution between two sliding windows of recent cases. When concept drift is detected, it also identifies the competence area where the distribution changes most significantly; 2) Noise-Enhanced Fast Context Switch (NEFCS), which enhances the system's learning capacity under concept drift; 3) Stepwise Redundancy Removal (SRR) method which controls the size of the case-base to tackle the performance issue.
%
%Other research discusses the cost and availability of new labeled training data and proposes methods for handling concept drift with limited labeled instances~\cite{huang07, linds10, linds12}.
%


\section{Discretization for data streaming}

Discretization, like other techniques, is also affected by changes in data distribution. Number and structure of discretization intervals may change over time as stationarity is not guaranteed in streaming environments. Therefore, if there appears a drift in distribution it  may be desirable that discretization intervals also move following the concept drift.

Equal-frequency discretization (based on histograms computation) can be considered as one of the first techniques in dealing with incremental discretization for streaming. By using the quantiles as cut points, the feature space can be partitioned in equal-frequency intervals. Estimation of quantiles in streams have been studied in depth in the literature, in both forms: approximate~\cite{ben10, webb14} and exact~\cite{gupta03, guha09}. Equal-width discretizer is another unsupervised approach that only requires as input the range of features and the number of splitting intervals. The main drawback of both techniques is related to the requirement of streamed records arriving in random order, which is not possible in many learning problems. 

Another important requirement to be considered is that some incremental algorithms need to maintain the same set of cut points (number, structure and meaning) over time~\cite{webb14}. That is the case of most discriminative learning algorithms. For these algorithms, a equal-width or equal-frequency discretizer could fit as both defines as input parameter the number of bins and most of times maintain their meaning. Other static algorithms, like Naive Bayes, does not require the preservation of intervals during the subsequent predictive phase, but only to save some statistics for the current discretization scheme. 

According to~\cite{gama06}, one of the main problems of unsupervised discretizers is the necessity of defining the number of bins. Such decision can be assisted by some pre-defined rules (e.g., Sturges' rule) or by an exploratory analysis process. However, exploratory analysis is no longer possible in the present days where the number of instances is too large and the rules has shown to work only with small samples.

Here, we enumerate those alternatives focused on supervised discretization that automatically computes the number of splits:

\begin{itemize}
	\item Gama et al.~\cite{gama06} presented the Partition Incremental Discretization algorithm (PiD). The algorithm consists of two layers. The first one summarizes data and creates the preliminary intervals, which will be optimized in the next layer. An equal-width strategy can be used to initialize this layer. Then, the first layer is updated trough a splitting process whenever the number of elements in a interval is above a pre-defined threshold. The second layer performs a merging process over the previous phase in order to yield the final discretization scheme. Any discretization algorithm can be used in this layer since the intervals generated in the previous phase are used as input for the current discretizer.
	\item Related to equal-frequency discretization, Lu et al.~\cite{lu06} presented the Incremental Flexible Frequency Discretization (IFFD) algorithm. IFFD defines a range $[minBinsize, maxBinsize)$ instead of a strict number of quantiles. The split \& merge process is described as follows: if the updated interval's frequency reaches $maxBinsize$ and the resulting frequencies are not below $minBinSize$ (in order to prevent high classification variance), FFID splits the interval into two partitions. If not, there is no split and the updating process continues. The main advantage of IFFD is that its local updating process only affects a minimum number of intervals.
	\item In~\cite{lehti12}, an online version of ChiMerge that maintains the O($nlog(n)$) time complexity held by the original algorithm is proposed. In order to also guarantee equal discretization results, authors implements a window-based evaluation process with some new data structures, like a binary tree for the observed values. Despite of its effectiveness, a high increment in the memory usage (derived from the new data structures) is attached to this new version, which may prevent from its usage in some data stream scenarios.
\end{itemize}

\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Summary description of streaming discretization methods. Information about the type of discretization strategy and processing is shown here.}
\label{tab:fs}
\begin{tabular}{ lcc }
\toprule
{\bf Method} & {\bf Discretization strategy} & {\bf Type of processing}\\
\midrule
PiD~\cite{gama06} & split \& merge & incremental\\
IFFD~\cite{lu06} & split \& merge & incremental\\
Online ChiMerge~\cite{lehti12} & merge & window\\
\bottomrule
\end{tabular}
\end{table}

%Ben-Haim et al.~\cite{ben10}, presented an incremental and online discretization for decision trees. Their algorithm is based on three methods: update: add a new example. It can be done by inserting the new example directly in an existing histogram or create a new bin with it and then do a merge; merge: join two bins into only one; uniform: use a trapezoid method to build the final Equal Frequency bins. This method has a low computational requirement and is incremental but it introduces some errors. In case of skewed distributions the authors recommend to use bound error algorithms.

\section*{Experiments}

In this section we evaluate the usefulness and performance of the proposals presented before from different perspectives:

\begin{itemize}
	\item Effectiveness: measured as the number of correctly classified instances divided by the total number of instances in the training set (accuracy).
	\item Performance: measured as the total time spent by the algorithm in the reduction phase. This phase is usually performed before the learning phase, although sometimes it is performed simultaneously.
	\item Reduction rate: measured as the amount of information reduced with respect to the original set (in percentage). For selection methods, it is related to the number of rows/columns removed, whereas for discretization, it is related to the degree of simplification in the feature spaces.
\end{itemize} 

\subsection{Experimental framework: datasets, methods and configurations}

We evaluate the different proposals by grouping them in three different experimental environments according to their preprocessing task. The order followed is: feature selection, instance selection, and finally, discretization.

Table~\ref{tab:datasets} shows the complete list of artificial/real datasets, used in our experiments to assess usefulness and performance of preprocessing algorithms. Artificial stream datasets have been generated using MOA benchmark, aiming at providing a wide range of drifting environments --blips, sudden and gradual, among others-- to test reduction algorithms. Each artificial dataset has been created using different combinations of generators and parameters. For a complete description of datasets and associated code, please refer to our GitHub repository\footnote{\url{https://github.com/sramirez/MOAReduction}}. 

\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Relevant information about classification datasets. For each
element, the number of instances evaluated (\#Inst.), the number of attributes (\#Atts.) (which ones are numerical (\#Num.) and which ones nominal (\#Nom.)), the number of classes (\#Cl), and whether the dataset is artificially generated or not (artificial) are shown.}
\label{tab:datasets}
\resizebox{0.75\textwidth}{!}{
  \begin{tabular}{|l||c||c||c||c||c||c|}
  \hline {\bf Data Set} & {\bf \#Inst.} & {\bf \#Atts.} & {\bf \#Num.} & {\bf \#Nom.} &{\bf \#Cl.} & {\bf artificial}\\
  \hline
\textit{blips} & 500,000 & 20 & 20 & 0 & 4 & yes\\ 
\textit{fast\_sudden\_recurring} & 500,000 & 10 & 5 & 5 & 6 & yes\\ 
\textit{gradual\_drift} & 500,000 & 3 & 3 & 0 & 2 & yes\\ 
\textit{gradual\_gradual\_virtual} & 500,000 & 40 & 40 & 0 & 3 & yes\\ 
\textit{gradual\_recurring\_drift} & 500,000 & 20 & 20 & 0 & 4 & yes\\ 
\textit{incremental\_fast} & 500,000 & 10 & 10 & 0 & 4 & yes\\ 
\textit{incremental\_slow} & 500,000 & 10 & 10 & 0 & 4 & yes\\ 
\textit{no\_drift} & 500,000 & 24 & 0 & 24 & 10 & yes\\ 
\textit{sudden\_drift} & 500,000 & 3 & 3 & 0 & 2 & yes\\ 
\textit{sudden\_virtual} & 500,000 & 24 & 0 & 24 & 10 & yes\\ 
\textit{virtual\_drift} & 500,000 & 40 & 40 & 0 & 3 & yes\\ 
\textit{airlines} & 539,383 & 6 & 3 & 3 & 2 & no\\ 
\textit{covtypeNorm} & 581,011 & 54 & 10 & 44 & 7 & no\\ 
\textit{elecNormNew} & 45,311 & 8 & 7 & 1 & 2 & no \\ 
\textit{kddcup\_10} & 494,020 & 41 & 39 & 2 & 2 & no\\ 
\textit{poker-lsn} & 829,201 & 10 & 5 & 5 & 10 & no\\ 
\textit{spambase} & 4,601 & 57 & 57 & 0 & 2 & no\\ 
\textit{spam\_nominal} & 9,324 & 40,000 & 0 & 40,000 & 2 & no\\ 
\textit{usenet\_recurrent} & 5,931 & 659 & 0 & 659 & 2 & no\\ 
\textit{spam\_data} & 9,324 & 499 & 0 & 499 & 2 & no\\ 
\textit{usenet1} & 1,500 & 100 & 0 & 100 & 2 & no\\ 
\textit{usenet2} & 1,500 & 100 & 0 & 100 & 2 & no\\ 
\textit{usenet3} & 5997 & 27,893 & 0 & 27,893 & 2 & no\\ 
\textit{power\_supply} & 29,928 & 2 & 2 & 0 & 24 & no\\ 
  \hline
  \end{tabular}
}
\end{table}

Real datasets come from different sources. \textit{airlines}, \textit{elecNormNew}, \textit{poker-lsn}, and \textit{covtypeNorm} can be found in MOA's streams repository. \textit{spam\_data}, \textit{usenet1}, \textit{usenet2}, and \textit{usenet3} are mail datasets for text mining which suffers from concept drift\footnote{\url{http://mlkd.csd.auth.gr/concept_drift.html}}. \textit{spambase} is a collection of e-mails classified as spam collected from different users~\cite{bache13}. \textit{kddcup\_10}, \textit{spam\_nominal} (SpamAssasin), and \textit{usenet\_recurrent} were collected by the professor Gama and his research group KDUS\footnote{\url{http://www.liaad.up.pt/kdus/products/datasets-for-concept-drift}}. Last dataset (\textit{power\_supply}) comes from Stream Data Mining Repository\footnote{\url{http://www.cse.fau.edu/~xqzhu/stream.html}}, and contains power supply registers collected hourly of an electricity company.

Not all datasets in this list have been considered for every experiment, as each algorithm has its particularities and conditions. For instance, most of feature selectors require discrete features as they are based on mutual information. However, all MOA generators~\cite{bifet10} only generate datasets with continuous attributes. Thus, the final election of datasets and any detail concerned to their features will be described in further sections.

No previous fixed partitioning has been performed on these datasets, instead an online evaluation approach has been used to asses the quality of methods, which is known as an interleaved test-then-train. This technique, proposed by Bifet et al. in~\cite{bifet09}, proposes a model in which each example/batch (arriving at time $t$) is evaluated against $t-1$-model, and then it serves as input for training the same model in order to produce and updated $t$-model. Interleaved model arises as more accurate as it never test models on already seen elements, however it implies much more evaluations.

Preprocessing techniques are grouped by task in Table~\ref{tab:parameters}. The default parameter values has been set according to the authors' criterion.
Common parameters like window size or the number of initial elements to consider before starting the reduction process are equals for all methods in a group. A window size equals to one means that the algorithms works in an online way, whereas a value bigger than one implies a batch-based processing. For instance, feature selection and discretization methods follow an online perspective, whereas instance selectors mostly uses batches (but FISH or kNN).

\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Parameters of methods}
\label{tab:parameters}
  \resizebox{.75\textheight}{!}{
  \begin{tabular}{|l|l|}
  \hline
  \textbf{Method} & \textbf{Parameters} \\
  \hline
  \textbf{Feature selection} & window size = 1 (common)\\
  NB & --\\
  IG & --\\
  SI & --\\
  OFS & $\eta$ = 0.2, $\lambda$ = 0.01\\
  \hline
  \textbf{Discretization} & initial elements = 100, window size = 1 (common)\\
  NB & --\\
  OC &  --\\
  PiD & $\alpha$ = 0.75, initial bins = 500, instances to update layer \#2 = 10,000, min/max = 0/1\\
  \hline
  \textbf{Instance selection} & k = 3, window size = 100 (common)\\
  kNN & window size = 1\\  
  NEFCS-SRR & l = 10, pmax = 0.5, size limit = 1,000\\
  CBE& --\\
  ICF& --\\
  FISH & learner = kNN, distance proportion (time/space) = 0.5, window size = 1\\
  Learn++.NSE & learner = kNN, slope = 0.5, crossing point = 10, ensemble size = 15, pruning = no\\
  \hline
  \end{tabular}
  }
\end{table}

As most of feature selectors and discretizers are focused on Naive Bayes, it has been elected as reference for classification. Likewise, kNN serves as reference for instance selectors. Evaluation and training is performed unlikely for each task. Concerning feature selection, contingency tables (probabilities) are updated whenever an example arrives. During the testing phase, Naive Bayes makes predictions only considering the most relevant features. In discretization, contingency tables are also updated with new examples, although their structure may change when new intervals are generated. Prediction phase is performed as usual (no selection). However, a problem arises whenever new intervals are considered. New intervals will not have enough weight in prediction, whereas old intervals will be gradually forgotten.

For instance selection, we have used different update schemes depending on the original design of each algorithm and other special considerations. An instant update scheme, where new instances are immediately added to the case-base, has been adopted for kNN and FISH. For simple kNN, an ever-updated case-base tends to adapt well to drifts in concepts definition. FISH selects a different training set for each new examples to classify, so that an online model is again the best choice. 

In counterpart, NEFCS shows a batch-like behavior which requires two windows for drift detection. In this case, update of case-base is deferred and performed when the batch structure is full. For a fair comparison between competence models (CBE, ICF, NEFCS-SRR), we have adopted a model based on batches for all these algorithms. For CBE and ICF, new instances are instantly added to the case-base, but reduction is only performed when the batch size condition is met. Finally, Learn++.NSE adds to the ensemble a new classifier whenever a new batch arrives. 

The whole experimental environment has been executed in a single standard machine, with the following features: 2 processors Intel Core i7 CPU 930 (4 cores/8 threads, 2.8 GHz, 8 MB cache), 24 GB of DDR2 RAM, 1 TB SATA HDD (3 Gb/s), Ethernet network connection, CentOS 6.4 (Linux). All algorithms have been integrated in MOA software (16.04v) as an extension library~\footnote{\url{http://moa.cms.waikato.ac.nz/moa-extensions/}}. MOA has also served as benchmark for all experiments.

\subsection{Experimental analysis}

Once described the experimental framework and its design, we analyze the experimental results derived from it. Analysis have been divided in three sections according to the different data reductions tasks studied.

\subsubsection*{\textbf{Feature Selection}}

Here, we evaluate how well selection of relevant features is performed by the stream-based methods proposed in the literature. As most of these methods assumes features are discrete, we have only selected from Table~\ref{tab:datasets} those with only nominal attributes. Note that all these datasets are formed by emails, where each attribute represents the presence or the absence of a given word. These datasets fits well for feature selection as the corpus of words/features is normally quite large.

Firstly, in Table~\ref{tab:acc-fsel}, we measure the classification accuracy held by the three feature selectors considered in the experimental framework: IG, SI, and OFS; plus Naive Bayes, used as reference. From these results we can assert that NB yields better results when all features are available during prediction. Any of the feature selection methods considered is able to improve the accuracy. However, IG and SI yields results pretty close to the reference method, with the advantage of generating much simpler solutions (as we will see in Figure~\ref{fig:fsel}). 

Comparing selectors, we can note that information-based methods performs better than the OFS alternative based on feature weighting. Specially remarkable are \textit{spam\_data} and \textit{usenet\_recurrent} cases, where even with a significant reduction of features the perfect prediction can be obtained. It means that the algorithm is able to distingish between spam or legitimate e-mail with only ten words. Similar results can also be found in \textit{usenet1}, and \textit{usenet2}, which are skinny problems that only get reduced with the lowest selection scheme (10 features).  


\begin{table*}[!t]
  \caption{Final test accuracy by method (Feature Selection). The best outcome for
each dataset is highlighted in bold. The second row in header represents the number of feature selected. No selection is performed for Naive Bayes} \label{tab:acc-fsel}
  \centering
  \resizebox{1.\textwidth}{!}{
\begin{tabular}{|l|rrrrrrrrrr|}
\toprule
 & \multicolumn{1}{c}{\textbf{Naive Bayes}} & \multicolumn{3}{c}{\textbf{InfoGain}} & \multicolumn{3}{c}{\textbf{Symmetrical Uncertainty}} & \multicolumn{3}{c|}{\textbf{OFS}} \\
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} & \multicolumn{1}{c}{\textit{1,000}} & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} &
 \multicolumn{1}{c}{\textit{1,000}} & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} & \multicolumn{1}{c|}{\textit{1,000}}\\ \hline
\textit{spam\_data} & 90.6692 & 89.2750 & 90.8516 & 90.6692 & 88.9103 & 90.4333 & 90.6692 & 90.0579 & \textbf{91.7417} & 90.6692 \\ 
\textit{spam\_nominal} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} \\
\textit{usenet1} & \textbf{63.3333} & 53.6667 & \textbf{63.3333} & \textbf{63.3333} & 53.2667 & \textbf{63.3333} & \textbf{63.3333} & 58.3333 & \textbf{63.3333} & \textbf{63.3333} \\
\textit{usenet2} & \textbf{72.1333} & 66.9333 & \textbf{72.1333} & \textbf{72.1333} & 66.6667 & \textbf{72.1333} & \textbf{72.1333} & 68.2000 & \textbf{72.1333} & \textbf{72.1333} \\ 
\textit{usenet3} & \textbf{84.6038} & 68.8073 & 78.2319 & 82.9024 & 69.0242 & 77.8816 & 82.8691 & 54.0951 & 57.4646 & 70.5922 \\
\textit{usenet\_recurrent} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} \\ 
\textit{no\_drift} & 51.4120 & \textbf{51.4240} & 51.4120 & 51.4120 & \textbf{51.4240} & 51.4120 & 51.4120 & 32.5830 & 51.4120 & 51.4120 \\
\midrule
\textbf{MEAN} & \textbf{80.3074} & 75.7295 & 79.4232 & 80.0643 & 75.6131 & 79.3134 & 80.0596 & 71.8956 & 76.5836 & 78.3057 \\ 
     \bottomrule
\end{tabular}
	}
\end{table*}

To assert that no method is better than simple NB, we conduct an statistical analysis on classification results through a pair of non-parametric tests: Wilcoxon Signed-Ranks Test --pairwise comparison-- and Friedman-Holm Test --one vs. all--~\cite{garcia09, derrac11}. Wilcoxon Test conducts pairwise comparisons between the reference method and the rest. A level of significance $\alpha = 0.05$ has been chosen for this experiment. The first two columns in Table~\ref{tab:friedman-fsel} show Wilcoxon's results for accuracy, where '+' symbol indicates the number of methods outperformed by the algorithm in each row. Symbol '$\pm$' represents the number of wins and ties yielded by each method. The best value by column is highlighted by shaded background. The remaining columns show the results for the Friedman test. 

%This test generates a ranking according to the effectiveness associated with each discretizer (second column), ordering the tables from the best to the worst Friedman ranking. The third column shows the adjusted p-value with the post hoc Holm’s test. Note that EMD is established as the control algorithm because it has obtained the best position in all the rankings. By using a level of significance α = 0.1, EMD is significantly better than the rest of the methods, considering both C4.5 accuracy and number of cut points.



\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Wilcoxon test results and average rankings of methods (Friedman Procedure \& Adjusted p-value with Holm's Test) for accuracy}
\label{tab:friedman-fsel}
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{lcccc}
\hline
 Algorithms & \multicolumn{2}{c}{Accuracy} & Ranking & $p_{Holm}$ \\
 & $+$ & $\pm$ & & \\
\toprule
SI-1,000 & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 25 & -- \\ 
SI-100 & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 25.5 & 0.963339 \\ 
NaiveBayes & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 25.9286 & 0.931974 \\
InfoGain-1,000 & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 26.0714 & 0.92154\\ 
InfoGain-100 & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 26.1429 & 0.916328 \\ 
OFS-1,000 & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 33 & 0.462083 \\
OFS-100 & \cellcolor[gray]{0.8} 1 & \cellcolor[gray]{0.8} 9 & 33.6429 & 0.426894 \\
InfoGain-10 & 0 & \cellcolor[gray]{0.8} 9  & 50.7143 & 0.018086 \\ 
SI-10 & 0 & \cellcolor[gray]{0.8} 9 & 50.8571 & 0.017455 \\ 
OFS-10 & 0 & 2 & 58.1429 & 0.002313 \\ 
\hline
\end{tabular}
}
\end{table}


Figure~\ref{fig:fsel} depicts selection time spent by each algorithm, as well as the amount of reduction performed by each selection scheme (range from ten to one thousand features).
Despite Naive Bayes uses more variables to compute predictions, it performs faster since it does not require to compute feature importances.

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{experiments/results/plots/time_fsel}
  \caption{Selection time (in seconds)}
  \label{fig:time-fsel}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{experiments/results/plots/red_fsel}
  \caption{Reduction rate}
  \label{fig:red-fsel}
\end{subfigure}
\caption{Distribution representation for selection time and reduction rate (feature selection)}
\label{fig:fsel}
\end{figure}

%\begin{table*}[!t]
%  \caption{Selection time elapsed by method (Feature Selection)} \label{tab:time-fsel}
%  \centering
%  \resizebox{.75\textheight}{!}{
%\begin{tabular}{|l|rrrrrrrrrr|}
%\toprule
% & \multicolumn{1}{c}{\textbf{Naive Bayes}} & \multicolumn{3}{c}{\textbf{InfoGain}} & \multicolumn{3}{c}{\textbf{Symmetrical Uncertainty}} & \multicolumn{3}{c|}{\textbf{OFS}} \\
%  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} & \multicolumn{1}{c}{\textit{1,000}} & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} &
% \multicolumn{1}{c}{\textit{1,000}} & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} & \multicolumn{1}{c|}{\textit{1,000}}\\ \hline
%\textit{spam\_data} & 4.22 & 16.94 & 20.85 & 28.28 & 15.79 & 19.68 & 27.79 & 9.89 & \textbf{1.04} & 21.79 \\ 
%\textit{spam\_nominal} & 546.61 & 1,143.88 & 1,100.08 & 979.41 & 1,029.45 & 1,144.02 & 1,215.26 & 746.85 & \textbf{357.58} & 830.86 \\
%\textit{usenet1} & \textbf{0.41} & 1.42 & 2.37 & 2.11 & 1.44 & 2.11 & 2.21 & 1.07 & 5.16 & 1.25 \\
%\textit{usenet2} & \textbf{0.43} & 1.44 & 2.08 & 2.26 & 1.50 & 2.13 & 1.79 & 1.04 & 12.49 & 1.60 \\ 
%\textit{usenet3} & 277.80 & 501.81 & 505.50 & 590.49 & 561.33 & 525.37 & 678.62 & 357.58 & \textbf{12.88} & 362.54 \\
%\textit{usenet\_recurrent} & \textbf{2.84} & 7.88 & 8.11 & 13.19 & 7.73 & 8.88 & 13.62 & 5.16 & 819.22 & 10.43 \\ 
%\textit{no\_drift} & 2.60 & 30.63 & 24.06 & 27.27 & 30.32 & 25.73 & 22.44 & 12.49 & \textbf{1.66} & 12.83 \\
%\midrule
%\textbf{MEAN} & \textbf{119.27} & 243.43 & 237.58 & 234.72 & 235.37 & 246.85 & 280.25 & 162.01 & 172.86 & 177.33 \\ 
%     \bottomrule
%\end{tabular}
%	}
%\end{table*}





%\begin{table*}[!t]
%  \caption{Reduction rate performed by the different feature selection schemes} \label{tab:red-fsel}
%  \centering
%  \resizebox{.35\textheight}{!}{
%\begin{tabular}{|l|rrr|}
%\toprule
%  & \multicolumn{3}{c|}{\textbf{\# features selected}} \\
%  & \multicolumn{1}{c}{\textit{10}} & \multicolumn{1}{c}{\textit{100}} & \multicolumn{1}{c|}{\textit{1,000}} \\ \hline
%\textit{spam\_data} & \textbf{98.00} & 79.96 & 0.00 \\ 
%\textit{spam\_nominal} & \textbf{99.98} & 99.75 & 97.50 \\
%\textit{usenet1} & \textbf{90.00} & 0.00 & 0.00 \\
%\textit{usenet2} & \textbf{90.00} & 0.00 & 0.00 \\ 
%\textit{usenet3} & \textbf{99.96} & 99.64 & 96.41 \\
%\textit{usenet\_recurrent} & \textbf{98.48} & 84.83 & 0.00 \\ 
%\textit{no\_drift} & \textbf{58.33} & 0.00 & 0.00 \\
%\midrule
%\textbf{MEAN} & \textbf{90.68} & 52.03 & 27.70 \\ 
%\bottomrule
%\end{tabular}
%	}
%\end{table*}



\subsubsection{Instance Selection}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{experiments/results/plots/time_instance}
  \caption{Selection time (in seconds)}
  \label{fig:time-isel}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{experiments/results/plots/inst_inst}
  \caption{\# instances selected by method}
  \label{fig:inst-isel}
\end{subfigure}
\caption{Distribution representation for time and \# instances selected (instance selection)}
\label{fig:isel}
\end{figure}

%
%\begin{table*}[!t]
%  \caption{Selection time elapsed by method (Instance Selection)} \label{tab:time-isel}
%  \centering
%  \resizebox{.75\textheight}{!}{
%\begin{tabular}{|l|rrrrrr|}
%\toprule
% & \multicolumn{1}{l}{\textbf{NEFCSSRR}} & \multicolumn{1}{l}{\textbf{ICF}} & \multicolumn{1}{l}{\textbf{BBNR}} & \multicolumn{1}{l}{\textbf{FISH}} & \multicolumn{1}{l}{\textbf{LearnNSE}} & \multicolumn{1}{l|}{\textbf{kNN}} \\ \hline
%\textit{elecNormNew} & 2,910.07 & 2,643.77 & 2,913.84 & 2,618.02 & 702.55 & \textbf{16.74} \\
%\textit{powersupply} & 2,838.86 & 2,416.89 & 2,678.02 & 2,952.24 & 53.84 & \textbf{7.02} \\
%\textit{spambase} & 698.46 & 1,495.58 & 7,077.14 & 141.85 & 46.39 & \textbf{11.00} \\
%\textit{spam\_data} & 2,879.77 & 2,598.12 & 22,562.15 & 5,972.89 & 2,672.26 & \textbf{355.65} \\
%\textit{spam\_nominal} & 2,981.09 & 1,917.74 & \textbf{1,410.35} & 2,588.89 & 2,527.07 & 2,566.37 \\
%\textit{usenet1} & 60.81 & 27.30 & 510.25 & \textbf{3.77} & 12.83 & 7.47 \\
%\textit{usenet2} & 55.13 & 29.56 & 84.61 & \textbf{3.63} & 10.95 & 7.22 \\
%\textit{usenet\_recurrent} & 1,438.81 & \textbf{99.25} & 3,618.86 & 2,442.95 & 1,688.52 & 350.14 \\
%\textit{blips} & 2,725.08 & 3,245.68 & 1,762.62 & 1,461.55 & 50.02 & \textbf{9.69} \\
%\textit{sudden\_drift} & 2,065.31 & 10,246.86 & 1,449.81 & 1,369.90 & 14.83 & \textbf{4.31} \\
%\textit{gradual\_drift} & 2,957.79 & 7,038.86 & 1,577.92 & 1,354.11 & 18.35 & \textbf{3.66} \\
%\textit{gradual\_recurring\_drift} & 2,535.35 & 2,996.82 & 1,108.70 & 1,391.40 & 53.49 & \textbf{10.33} \\
%\textit{incremental\_fast} & 2,772.65 & 6,853.68 & 1,289.74 & 1,365.93 & 46.19 & \textbf{7.75} \\
%\textit{incremental\_slow} & 2,343.20 & 7,383.19 & 1,750.39 & 1,401.78 & 40.02 & \textbf{8.07} \\
%\midrule
%\textbf{MEAN} & 2,090.17 & 3,499.52 & 3,556.74 & 1,790.64 & 566.95 & \textbf{240.39} \\
%\bottomrule
%\end{tabular}
%	}
%\end{table*}



\begin{table*}[!t]
  \caption{Total test accuracy by method (Instance Selection)} \label{tab:acc-isel}
  \centering
  \resizebox{.75\textheight}{!}{
\begin{tabular}{|l|rrrrrr|}
\toprule
 & \multicolumn{1}{l}{\textbf{NEFCSSRR}} & \multicolumn{1}{l}{\textbf{ICF}} & \multicolumn{1}{l}{\textbf{BBNR}} & \multicolumn{1}{l}{\textbf{FISH}} & \multicolumn{1}{l}{\textbf{LearnNSE}} & \multicolumn{1}{l|}{\textbf{kNN}} \\ \hline
\textit{elecNormNew} & 67.7652 & 43.7882 & 73.8105 & 60.0455 & 69.6438 & \textbf{84.0815} \\ 
\textit{powersupply} & 11.9400 & 4.2502 & 12.3800 & 5.4435 & 8.7878 & \textbf{15.1296} \\ 
\textit{spambase} & 81.6779 & 39.3827 & \textbf{97.5440} & 95.9139 & 57.8570 & 95.1532 \\ 
\textit{spam\_data} & 88.8782 & 25.6113 & 91.1197 & 77.3488 & 73.1338 & 93.9833 \\ 
\textit{spam\_nominal} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} \\ 
\textit{usenet1} & \textbf{56.6667} & 54.5333 & 54.0667 & 55.2000 & 52.7333 & 56.4667 \\ 
\textit{usenet2} & 61.2000 & 63.4667 & 48.4000 & \textbf{69.8000} & 62.7333 & 68.2000 \\ 
\textit{usenet\_recurrent} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} & \textbf{100.0000} \\ 
\textit{blips} & 90.8900 & 34.1300 & 94.2300 & 31.3200 & 82.1400 & \textbf{97.1800} \\ 
\textit{sudden\_drift} & 76.5300 & 61.7300 & 74.2300 & 60.8700 & 79.3700 & \textbf{82.6600} \\ 
\textit{gradual\_drift} & 68.2700 & 52.3600 & 74.4500 & 52.0200 & 79.3900 & \textbf{81.2700} \\ 
\textit{gradual\_recurring\_drift} & 87.5800 & 28.9400 & 92.6500 & 28.8400 & 77.9700 & \textbf{96.3300} \\ 
\textit{incremental\_fast} & 65.8900 & 51.7700 & 68.4000 & 55.8300 & 75.2800 & \textbf{77.2800} \\ 
\textit{incremental\_slow} & 72.4300 & 50.9800 & 68.7000 & 56.5800 & 76.7600 & \textbf{79.1000} \\ 
\midrule
\textbf{MEAN} & 73.5513 & 50.7816 & 74.9986 & 60.6580 & 71.1285 & \textbf{80.4882} \\ 
\bottomrule
\end{tabular}
	}
\end{table*}

%\begin{figure*}[!thp]
%  \begin{center}
%    \includegraphics[width=0.75\textwidth]{experiments/results/plots/inst_inst}
%  \caption{Final number of instances selected by each method (distribution)}
%  \label{fig:time-fsel}
%  \end{center}
%\end{figure*}

%\begin{table*}[!t]
%  \caption{\# instances selected by method (Instance Selection)} \label{tab:inst-isel}
%  \centering
%  \resizebox{.75\textheight}{!}{
%\begin{tabular}{|l|rrrrrr|}
%\toprule
% & \multicolumn{1}{l}{\textbf{NEFCSSRR}} & \multicolumn{1}{l}{\textbf{ICF}} & \multicolumn{1}{l}{\textbf{BBNR}} & \multicolumn{1}{l}{\textbf{FISH}} & \multicolumn{1}{l}{\textbf{LearnNSE}} & \multicolumn{1}{l|}{\textbf{kNN}} \\ \hline
%\textit{elecNormNew} & 1,558 & 3,152 & \textbf{238} & 45,312 & - & 45,312 \\ 
%\textit{powersupply} & 3,349 & \textbf{1,215} & 3,569 & 29,928 & - & 29,928 \\ 
%\textit{spambase} & 905 & 1,813 & \textbf{2} & 4,601 & - & 4,601 \\ 
%\textit{spam\_data} & 927 & 2,087 & \textbf{5} & 9,324 & - & 9,324 \\ 
%\textit{spam\_nominal} & 934 & 2,001 & \textbf{1} & 800 & - & 800 \\ 
%\textit{usenet1} & 894 & 311 & \textbf{269} & 1,500 & - & 1,500 \\ 
%\textit{usenet2} & 903 & 378 & \textbf{334} & 1,500 & - & 1,500 \\ 
%\textit{usenet\_recurrent} & 990 & \textbf{1} & 5,501 & 5,931 & - & 5,931 \\ 
%\textit{blips} & 802 & 2,947 & \textbf{57} & 10,000 & - & 10,000 \\ 
%\textit{sudden\_drift} & 824 & 5,540 & \textbf{227} & 10,000 & - & 10,000 \\ 
%\textit{gradual\_drift} & 1,584 & 4,547 & \textbf{190} & 10,000 & - & 10,000 \\ 
%\textit{gradual\_recurring\_drift} & 772 & 2,508 & \textbf{85} & 10,000 & - & 10,000 \\ 
%\textit{incremental\_fast} & 1,492 & 4,651 & \textbf{244} & 10,000 & - & 10,000 \\ 
%\textit{incremental\_slow} & 1,298 & 4,539 & \textbf{243} & 10,000 & - & 10,000 \\ 
%\midrule
%\textbf{MEAN} & 1,230.86 & 2,549.29 & \textbf{783.21} & 11,349.71 & - & 11,349.71 \\ 
%\bottomrule
%\end{tabular}
%	}
%\end{table*}

\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Wilcoxon test results and average rankings of methods (Friedman Procedure \& Adjusted p-value with Holm's Test) for accuracy}
\label{tab:wilcoxon-isel}
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{lcccccc}
\hline
 Algorithms & \multicolumn{2}{c}{Accuracy} & Ranking & $p_{Holm}$ \\
 & $+$ & $\pm$ & & \\
\toprule
kNN & \cellcolor[gray]{0.8} 5 & \cellcolor[gray]{0.8} 5 & 21 & --\\
BBNR & 2 & 4 & 32.6429 & 0.265669\\
NEFCSSRR & 2 & 4 & 34.8571 & 0.265669\\
LearnNSE & 1 & 4 & 40.5 & 0.103271\\
FISH & 0 & 2 & 57 & 0.000377\\
ICF & 0 & 1 & 69 & 0.000001\\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection*{Discretization}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{experiments/results/plots/time_disc}
  \caption{Selection time (in seconds)}
  \label{fig:time-disc}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{experiments/results/plots/intervals_disc}
  \caption{\# intervals generated by method}
  \label{fig:interv-disc}
\end{subfigure}
\caption{Distribution representation for time and \# intervals}
\label{fig:disc}
\end{figure}


\begin{table*}[!t]
  \caption{Discretization time, test accuracy and number of intervals results for all discretizers} \label{tab:res-disc}
  \centering
  \resizebox{.75\textheight}{!}{
\begin{tabular}{|l|rrrrrrrr|}
\toprule
& \multicolumn{3}{c}{\textbf{Time}} & \multicolumn{3}{c}{\textbf{Accuracy}} & \multicolumn{2}{c|}{\textbf{\# intervals}} \\
 & \multicolumn{1}{l}{\textbf{PiD}} & \multicolumn{1}{l}{\textbf{OC}} & \multicolumn{1}{l}{\textbf{Naive Bayes}} & \multicolumn{1}{l}{\textbf{PiD}} & \multicolumn{1}{l}{\textbf{OC}} & \multicolumn{1}{l}{\textbf{Naive Bayes}} & \multicolumn{1}{l}{\textbf{PiD}} & \multicolumn{1}{l|}{\textbf{OC}} \\ \hline
\textit{airlines} & 17.53 & 286.57 & \textbf{14.04} & 63.5817 & 63.9015 & \textbf{64.5504} & \textbf{19} & 33 \\ 
\textit{powersupply} & 1.11 & 1.31 & \textbf{0.64} & 63.5817 & 63.9015 & \textbf{64.5504} & 1,018 & \textbf{14} \\ 
\textit{elecNormNew} & 1.52 & 7.46 & \textbf{0.67} & 42.4545 & 57.1306 & \textbf{73.3625} & 2,041 & \textbf{49} \\ 
\textit{spambase} & 0.96 & 1.30 & \textbf{0.50} & \textbf{87.8939} & 84.2426 & 82.8081 & 29,116 & \textbf{228}\\ 
\textit{kddcup\_10} & 29.78 & 3,317.92 & \textbf{18.59}  & 20.1164 & 95.4200 & \textbf{97.1908} & 273 & \textbf{418}\\ 
\textit{poker-lsn} & 17.29 & 2,280.30 & \textbf{11.72} & 2.6008 & 52.3943 & \textbf{59.5528} & \textbf{54} & 55\\ 
\textit{covtypeNorm} & 38.13 & 888.83 & \textbf{28.28} & 54.0197 & 3.1984 & \textbf{60.5208} & \textbf{74} & 110\\ 
\textit{blips} & 18.29 & 1,927.32 & \textbf{12.02} & 30.3674 & 26.7278 & \textbf{60.9060} & \textbf{92} & 220 \\ 
\textit{sudden\_drift} & 6.05 & 229.00 & \textbf{2.49} & 59.3210 & 78.4012 & \textbf{83.8144} & \textbf{10} & 33 \\ 
\textit{gradual\_drift} & 6.09 & 421.11 & \textbf{3.63} & 52.1964 & 76.2868 & \textbf{84.7000} & \textbf{10} & 33\\ 
\textit{gradual\_recurring\_drift} & 19.52 & 1,722.27 & \textbf{12.52} & 26.9604 & 28.5176 & \textbf{56.7450} & \textbf{100} & 220 \\ 
\textit{incremental\_fast} & 12.24 & 1,039.89 & \textbf{5.68} & 49.9620 & 49.9460 & \textbf{76.3642} & 4,608 & \textbf{110}\\ 
\textit{incremental\_slow} & 13.41 & 987.31 & \textbf{6.06} & 49.9966 & 49.9866 & \textbf{78.0688} & 5,120 & \textbf{110}\\ 
\midrule
\textbf{MEAN} & 13.99 & 1,008.51 & \textbf{8.99} & 41.7651 & 51.3328 & \textbf{68.8225} & 3,271.92 & \textbf{125.62} \\ 
\bottomrule
\end{tabular}
	}
\end{table*}


\begin{table}[!htp]
\renewcommand{\arraystretch}{1.3}
\centering
\scriptsize
\caption{Wilcoxon test results and average rankings of methods (Friedman Procedure \& Adjusted p-value with Holm's Test) for accuracy}
\label{tab:wilcoxon-disc}
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{lcccccc}
\hline
 Algorithms & \multicolumn{2}{c}{Accuracy} & Ranking & $p_{Holm}$ \\
 & $+$ & $\pm$ & & \\
\toprule
Naive Bayes & \cellcolor[gray]{0.8} 2 & \cellcolor[gray]{0.8} 2 & 9.4615 & --\\
OC & 0 & 1 & 22.1538 & 0.004538\\
PiD & 0 & 1 & 28.3846 & 0.000023\\
\bottomrule
\end{tabular}
}
\end{table}


\section*{Acknowlegments}

This work is supported by the Spanish National Research Project TIN2012-37954, TIN2013-47210-P and TIN2014-57251-P, and the Andalusian Research Plan P10-TIC-6858, P11-TIC-7765 and P12-TIC-2958. S. Ram\'irez-Gallego holds a FPU scholarship from the Spanish Ministry of Education and Science (FPU13/00047).

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num} 
\bibliography{biblio}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%\begin{thebibliography}{00}
%
%%% \bibitem{label}
%%% Text of bibliographic item
%
%\bibitem{}
%
%\end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
